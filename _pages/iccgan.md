---
permalink: /ICCGAN
title: "A GAN-Like Approach for Physics-Based Imitation Learning and Interactive Character Control"
excerpt: ""
author_profile: false
redirect_from: 
 - /iccgan
--- 


# Context-Aware Timewise VAEs for Real-Time Vehicle Trajectory Prediction

<p class="author">
Pei Xu<sup>1</sup>, Ioannis Karamouzas<sup>1</sup>
</p>

<p class="affiliation">
<sup>1</sup> Clemson University
</p>


In _ACM Transactions on Graphics (Proceedings of SIGGRAPH 2023)_

<div class="teaser">
<p><img src="projects/ICCGAN/teaser_fight.png" /></p>
</div>

<div class="m10"></div>
## Abstract
We present a simple and intuitive approach for interactive control of physically simulated characters. Our work builds upon generative adversarial networks (GAN) and reinforcement learning, and introduces an imitation learning framework where an ensemble of classifiers and an imitation policy are trained in tandem given pre-processed reference clips. The classifiers are trained to discriminate the reference motion from the motion generated by the imitation policy, while the policy is rewarded for fooling the discriminators. Using our GAN-based approach, multiple motor control policies can be trained separately to imitate different behaviors. In runtime, our system can respond to external control signal provided by the user and interactively switch between different policies. Compared to existing methods, our proposed approach has the following attractive properties: 1) achieves state-of-the-art imitation performance without manually designing and fine tuning a reward function; 2) directly controls the character without having to track any target reference pose explicitly or implicitly through a phase state; and 3) supports interactive policy switching without requiring any motion generation or motion matching mechanism. We highlight the applicability of our approach in a range of imitation and interactive control tasks, while also demonstrating its ability to withstand external perturbations as well as to recover balance. Overall, our approach generates high-fidelity motion, has low runtime cost, and can be easily integrated into interactive applications and games.


<div class="m10"></div>
<a class="paper-link" href="https://arxiv.org/abs/2103.10000" title="Paper"></a>
<a class="code-link" href="https://github.com/xupei0610/CompositeMotion" title="Code"></a>

<div class="m10"></div>
## Video
<div style="max-width:560px">
<iframe width="560" height="315" src="https://www.youtube.com/embed/tMctyEw8kRI" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
</div>

<div class="m10"></div>
## Bibtex
<pre class="bibtex">
@article{composite,
    author = {Xu, Pei and Shang, Xiumin and Zordan, Victor and Karamouzas, Ioannis},
    title = {Composite Motion Learning with Task Control},
    journal = {ACM Transactions on Graphics},
    publisher = {ACM New York, NY, USA},
    year = {2023},
    volume = {42},
    number = {4},
    doi = {10.1145/3592447}
}
</pre>

